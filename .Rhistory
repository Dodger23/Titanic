type = "C-classification",
kernel = "linear"
)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#support_vectors_machine_model = support_vectors_machine(training , testing)
support_vectors_machine_model
random_forst = function(training , testing )
{
# Saving a call to the trainControl() with method = Cross Validation
tCall = trainControl(method = "cv" , number  = 5)
# Training the model on the training data with random forest and Cross Validation
modelFit = train(Survived ~ . , method = "rf" , data = training , trControl = tCall)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#random_forest_model = random_forst(training , testing )
random_forest_model
naive_bayes = function(training , testing )
{
# Training the model on the training data with naive bayes and enabling laplace for smoothing effect
modelFit = naiveBayes(Survived ~ . , method = "nb" , data = training  , laplace = 1)
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#naive_bayes_model = naive_bayes(training , testing )
naive_bayes_model
decision_tree = function(training , testing )
{
# Training the model on the training data
modelFit  = rpart(Survived ~ . , data = training)
modelFit
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , newdata = testing)
#print(c(length(pred[,1]) , nrow(testing) ))
#print(class(pred[,1]))
p = vector()
for(i in 1:length(pred[,2]))
{
if(pred[i , 1] >= pred[i, 2])
p = c(p , 0)
else
p = c(p , 1)
}
print(p)
#print(confusionMatrix(p , testing$Survived))
#confusionMatrix(p , testing$Survived)$overall[1] *100
}
decision_tree_model = decision_tree(training , testing )
decision_tree_model
library(caret)
library(ROSE)
library(rpart)
library(e1071)
set.seed(333)
#Reading training and Quiz data
data = read.csv("data/train.csv" , na.strings = c("NA" , ""))
TEST = read.csv("data/test.csv" , na.strings = c("NA" ,""))
clean = function(data)
{
data$Survived = as.factor(data$Survived)
c= c("Name" , "PassengerId"  , "Cabin")
data = data[, -which(names(data) %in% c )]
if(sum(is.na(data$Age)) > 0)
data[is.na(data$Age) , "Age"] = median(data$Age , na.rm = TRUE)
if(sum(is.na(data$Embarked)) > 0)
data[is.na(data$Embarked) , "Embarked"] = levels(data$Embarked)[runif(1 , 1 , 3)]
if(sum(is.na(data$Fare)) > 0)
data[is.na(data$Fare) , "Fare"] = median(data$Fare , na.rm = TRUE)
data
}
explore = function (data)
{
str(data)
# Find number of NA's in each variable
find_na = apply(is.na(data),2,sum)
# Find the percentage of NA's in each variable
find_na_percent = apply(is.na(data) , 2 , sum  ) / nrow(data) *100
# Saving the names of variables which contain NA values
c = names(find_na)[find_na > 0]
# Print names of each variable that contains NA values and the nummber of NA values in it
print(find_na[ which(names(find_na) %in% c )] )
# Print names of each variable that contains NA values and the percentage of NA values in it
print(find_na_percent[ which(names(find_na_percent) %in% c )])
# Cleaning the data
data = clean(data)
# Plotting relations bettween variables to find patterns
png(filename = "images/featuresPlot.png" , width = 1366 , height = 768 , units = "px")
p = featurePlot(x = data[,-which(names(data) %in% c("Ticket" , "Survived"))] ,y = data[,1] ,
plot = "pairs" ,
auto.key = list(columns = 2)
)
print(p)
dev.off()
}
explore(data)
# Splitting data to training and testing data
inTrain = createDataPartition(data$Survived , p = 0.75 , list = FALSE)
training = data[inTrain , ]
testing = data[-inTrain , ]
#cleaning training and testing
training = clean(training)
testing  = clean(testing)
support_vectors_machine = function (training , testing)
{
# Training the model using support vectors machoine with a linear kernel
modelFit = svm(Survived ~ . ,
data = training ,
type = "C-classification",
kernel = "linear"
)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#support_vectors_machine_model = support_vectors_machine(training , testing)
support_vectors_machine_model
random_forst = function(training , testing )
{
# Saving a call to the trainControl() with method = Cross Validation
tCall = trainControl(method = "cv" , number  = 5)
# Training the model on the training data with random forest and Cross Validation
modelFit = train(Survived ~ . , method = "rf" , data = training , trControl = tCall)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#random_forest_model = random_forst(training , testing )
random_forest_model
naive_bayes = function(training , testing )
{
# Training the model on the training data with naive bayes and enabling laplace for smoothing effect
modelFit = naiveBayes(Survived ~ . , method = "nb" , data = training  , laplace = 1)
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#naive_bayes_model = naive_bayes(training , testing )
naive_bayes_model
decision_tree = function(training , testing )
{
# Training the model on the training data
modelFit  = rpart(Survived ~ . , data = training)
modelFit
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , newdata = testing)
#print(c(length(pred[,1]) , nrow(testing) ))
#print(class(pred[,1]))
p = vector()
for(i in 1:length(pred[,2]))
{
if(pred[i , 1] >= pred[i, 2])
p = c(p , 0)
else
p = c(p , 1)
}
print(confusionMatrix(p , testing$Survived))
confusionMatrix(p , testing$Survived)$overall[1] *100
}
decision_tree_model = decision_tree(training , testing )
decision_tree_model
library(caret)
library(ROSE)
library(rpart)
library(e1071)
set.seed(333)
#Reading training and Quiz data
data = read.csv("data/train.csv" , na.strings = c("NA" , ""))
TEST = read.csv("data/test.csv" , na.strings = c("NA" ,""))
clean = function(data)
{
data$Survived = as.factor(data$Survived)
c= c("Name" , "PassengerId"  , "Cabin")
data = data[, -which(names(data) %in% c )]
if(sum(is.na(data$Age)) > 0)
data[is.na(data$Age) , "Age"] = median(data$Age , na.rm = TRUE)
if(sum(is.na(data$Embarked)) > 0)
data[is.na(data$Embarked) , "Embarked"] = levels(data$Embarked)[runif(1 , 1 , 3)]
if(sum(is.na(data$Fare)) > 0)
data[is.na(data$Fare) , "Fare"] = median(data$Fare , na.rm = TRUE)
data
}
explore = function (data)
{
str(data)
# Find number of NA's in each variable
find_na = apply(is.na(data),2,sum)
# Find the percentage of NA's in each variable
find_na_percent = apply(is.na(data) , 2 , sum  ) / nrow(data) *100
# Saving the names of variables which contain NA values
c = names(find_na)[find_na > 0]
# Print names of each variable that contains NA values and the nummber of NA values in it
print(find_na[ which(names(find_na) %in% c )] )
# Print names of each variable that contains NA values and the percentage of NA values in it
print(find_na_percent[ which(names(find_na_percent) %in% c )])
# Cleaning the data
data = clean(data)
# Plotting relations bettween variables to find patterns
png(filename = "images/featuresPlot.png" , width = 1366 , height = 768 , units = "px")
p = featurePlot(x = data[,-which(names(data) %in% c("Ticket" , "Survived"))] ,y = data[,1] ,
plot = "pairs" ,
auto.key = list(columns = 2)
)
print(p)
dev.off()
}
explore(data)
# Splitting data to training and testing data
inTrain = createDataPartition(data$Survived , p = 0.75 , list = FALSE)
training = data[inTrain , ]
testing = data[-inTrain , ]
#cleaning training and testing
training = clean(training)
testing  = clean(testing)
support_vectors_machine = function (training , testing)
{
# Training the model using support vectors machoine with a linear kernel
modelFit = svm(Survived ~ . ,
data = training ,
type = "C-classification",
kernel = "linear"
)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#support_vectors_machine_model = support_vectors_machine(training , testing)
support_vectors_machine_model
random_forst = function(training , testing )
{
# Saving a call to the trainControl() with method = Cross Validation
tCall = trainControl(method = "cv" , number  = 5)
# Training the model on the training data with random forest and Cross Validation
modelFit = train(Survived ~ . , method = "rf" , data = training , trControl = tCall)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#random_forest_model = random_forst(training , testing )
random_forest_model
naive_bayes = function(training , testing )
{
# Training the model on the training data with naive bayes and enabling laplace for smoothing effect
modelFit = naiveBayes(Survived ~ . , method = "nb" , data = training  , laplace = 1)
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#naive_bayes_model = naive_bayes(training , testing )
naive_bayes_model
decision_tree = function(training , testing )
{
# Training the model on the training data
modelFit  = rpart(Survived ~ . , data = training)
modelFit
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , newdata = testing)
p = vector()
for(i in 1:length(pred[,2]))
{
if(pred[i , 1] >= pred[i, 2])
p = c(p , 0)
else
p = c(p , 1)
}
print(c(length(p) , nrow(testing) ))
#print(confusionMatrix(p , testing$Survived))
#confusionMatrix(p , testing$Survived)$overall[1] *100
}
decision_tree_model = decision_tree(training , testing )
decision_tree_model
library(caret)
library(ROSE)
library(rpart)
library(e1071)
set.seed(333)
#Reading training and Quiz data
data = read.csv("data/train.csv" , na.strings = c("NA" , ""))
TEST = read.csv("data/test.csv" , na.strings = c("NA" ,""))
clean = function(data)
{
data$Survived = as.factor(data$Survived)
c= c("Name" , "PassengerId"  , "Cabin")
data = data[, -which(names(data) %in% c )]
if(sum(is.na(data$Age)) > 0)
data[is.na(data$Age) , "Age"] = median(data$Age , na.rm = TRUE)
if(sum(is.na(data$Embarked)) > 0)
data[is.na(data$Embarked) , "Embarked"] = levels(data$Embarked)[runif(1 , 1 , 3)]
if(sum(is.na(data$Fare)) > 0)
data[is.na(data$Fare) , "Fare"] = median(data$Fare , na.rm = TRUE)
data
}
explore = function (data)
{
str(data)
# Find number of NA's in each variable
find_na = apply(is.na(data),2,sum)
# Find the percentage of NA's in each variable
find_na_percent = apply(is.na(data) , 2 , sum  ) / nrow(data) *100
# Saving the names of variables which contain NA values
c = names(find_na)[find_na > 0]
# Print names of each variable that contains NA values and the nummber of NA values in it
print(find_na[ which(names(find_na) %in% c )] )
# Print names of each variable that contains NA values and the percentage of NA values in it
print(find_na_percent[ which(names(find_na_percent) %in% c )])
# Cleaning the data
data = clean(data)
# Plotting relations bettween variables to find patterns
png(filename = "images/featuresPlot.png" , width = 1366 , height = 768 , units = "px")
p = featurePlot(x = data[,-which(names(data) %in% c("Ticket" , "Survived"))] ,y = data[,1] ,
plot = "pairs" ,
auto.key = list(columns = 2)
)
print(p)
dev.off()
}
explore(data)
# Splitting data to training and testing data
inTrain = createDataPartition(data$Survived , p = 0.75 , list = FALSE)
training = data[inTrain , ]
testing = data[-inTrain , ]
#cleaning training and testing
training = clean(training)
testing  = clean(testing)
support_vectors_machine = function (training , testing)
{
# Training the model using support vectors machoine with a linear kernel
modelFit = svm(Survived ~ . ,
data = training ,
type = "C-classification",
kernel = "linear"
)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#support_vectors_machine_model = support_vectors_machine(training , testing)
support_vectors_machine_model
random_forst = function(training , testing )
{
# Saving a call to the trainControl() with method = Cross Validation
tCall = trainControl(method = "cv" , number  = 5)
# Training the model on the training data with random forest and Cross Validation
modelFit = train(Survived ~ . , method = "rf" , data = training , trControl = tCall)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#random_forest_model = random_forst(training , testing )
random_forest_model
naive_bayes = function(training , testing )
{
# Training the model on the training data with naive bayes and enabling laplace for smoothing effect
modelFit = naiveBayes(Survived ~ . , method = "nb" , data = training  , laplace = 1)
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#naive_bayes_model = naive_bayes(training , testing )
naive_bayes_model
decision_tree = function(training , testing )
{
# Training the model on the training data
modelFit  = rpart(Survived ~ . , data = training)
modelFit
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , newdata = testing)
p = vector()
for(i in 1:length(pred[,2]))
{
if(pred[i , 1] >= pred[i, 2])
p = c(p , 0)
else
p = c(p , 1)
}
print(c(length(p) , nrow(testing) ))
p = as.factor(p)
#print(confusionMatrix(p , testing$Survived))
#confusionMatrix(p , testing$Survived)$overall[1] *100
}
decision_tree_model = decision_tree(training , testing )
decision_tree_model
library(caret)
library(ROSE)
library(rpart)
library(e1071)
set.seed(333)
#Reading training and Quiz data
data = read.csv("data/train.csv" , na.strings = c("NA" , ""))
TEST = read.csv("data/test.csv" , na.strings = c("NA" ,""))
clean = function(data)
{
data$Survived = as.factor(data$Survived)
c= c("Name" , "PassengerId"  , "Cabin")
data = data[, -which(names(data) %in% c )]
if(sum(is.na(data$Age)) > 0)
data[is.na(data$Age) , "Age"] = median(data$Age , na.rm = TRUE)
if(sum(is.na(data$Embarked)) > 0)
data[is.na(data$Embarked) , "Embarked"] = levels(data$Embarked)[runif(1 , 1 , 3)]
if(sum(is.na(data$Fare)) > 0)
data[is.na(data$Fare) , "Fare"] = median(data$Fare , na.rm = TRUE)
data
}
explore = function (data)
{
str(data)
# Find number of NA's in each variable
find_na = apply(is.na(data),2,sum)
# Find the percentage of NA's in each variable
find_na_percent = apply(is.na(data) , 2 , sum  ) / nrow(data) *100
# Saving the names of variables which contain NA values
c = names(find_na)[find_na > 0]
# Print names of each variable that contains NA values and the nummber of NA values in it
print(find_na[ which(names(find_na) %in% c )] )
# Print names of each variable that contains NA values and the percentage of NA values in it
print(find_na_percent[ which(names(find_na_percent) %in% c )])
# Cleaning the data
data = clean(data)
# Plotting relations bettween variables to find patterns
png(filename = "images/featuresPlot.png" , width = 1366 , height = 768 , units = "px")
p = featurePlot(x = data[,-which(names(data) %in% c("Ticket" , "Survived"))] ,y = data[,1] ,
plot = "pairs" ,
auto.key = list(columns = 2)
)
print(p)
dev.off()
}
explore(data)
# Splitting data to training and testing data
inTrain = createDataPartition(data$Survived , p = 0.75 , list = FALSE)
training = data[inTrain , ]
testing = data[-inTrain , ]
#cleaning training and testing
training = clean(training)
testing  = clean(testing)
support_vectors_machine = function (training , testing)
{
# Training the model using support vectors machoine with a linear kernel
modelFit = svm(Survived ~ . ,
data = training ,
type = "C-classification",
kernel = "linear"
)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#support_vectors_machine_model = support_vectors_machine(training , testing)
support_vectors_machine_model
random_forst = function(training , testing )
{
# Saving a call to the trainControl() with method = Cross Validation
tCall = trainControl(method = "cv" , number  = 5)
# Training the model on the training data with random forest and Cross Validation
modelFit = train(Survived ~ . , method = "rf" , data = training , trControl = tCall)
# Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#random_forest_model = random_forst(training , testing )
random_forest_model
naive_bayes = function(training , testing )
{
# Training the model on the training data with naive bayes and enabling laplace for smoothing effect
modelFit = naiveBayes(Survived ~ . , method = "nb" , data = training  , laplace = 1)
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , testing)
print(confusionMatrix(pred , testing$Survived))
confusionMatrix(pred , testing$Survived)$overall[1] *100
}
#naive_bayes_model = naive_bayes(training , testing )
naive_bayes_model
decision_tree = function(training , testing )
{
# Training the model on the training data
modelFit  = rpart(Survived ~ . , data = training)
modelFit
#Predicting on the testing data and showing accuracy
pred = predict(modelFit , newdata = testing)
p = vector()
for(i in 1:length(pred[,2]))
{
if(pred[i , 1] >= pred[i, 2])
p = c(p , 0)
else
p = c(p , 1)
}
#print(c(length(p) , nrow(testing) ))
p = as.factor(p)
print(confusionMatrix(p , testing$Survived))
confusionMatrix(p , testing$Survived)$overall[1] *100
}
decision_tree_model = decision_tree(training , testing )
decision_tree_model
